<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>System Design Interview – An insider&#39;s guide, Second Edition: Step by Step Guide, Tips and 15 System Design Interview Questions with Detailed Solutions</title><meta name="author" content="Alex Xu"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 16.5pt; }
 .p, p { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; margin:0pt; }
 h2 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 13.5pt; }
 .s1 { color: black; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 a { color: #1154CC; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 li {display: block; }
 #l1 {padding-left: 0pt; }
 #l1> li>*:first-child:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 li {display: block; }
 #l2 {padding-left: 0pt; }
 #l2> li>*:first-child:before {content: "• "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 li {display: block; }
 #l3 {padding-left: 0pt;counter-reset: e1 1; }
 #l3> li>*:first-child:before {counter-increment: e1; content: "["counter(e1, decimal)"] "; color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l3> li:first-child>*:first-child:before {counter-increment: e1 0;  }
</style></head><body><h1 style="padding-top: 3pt;padding-left: 48pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">CHAPTER 5: DESIGN CONSISTENT HASHING</a></h1><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">To achieve horizontal scaling, it is important to distribute requests/data efficiently and evenly across servers. Consistent hashing is a commonly used technique to achieve this goal. But first, let us take an in-depth look at the problem.</p><h2 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The rehashing problem</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">If you have <i>n </i>cache servers, a common way to balance the load is to use the following hash method:</p><p class="s1" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">serverIndex = hash(key) % N<span class="p">, where </span>N <span class="p">is the size of the server pool.</span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Let us use an example to illustrate how it works. As shown in Table 5-1, we have 4 servers and 8 string keys with their hashes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="444" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_001.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">To fetch the server where a key is stored, we perform the modular operation <i>f(key) % 4</i>. For instance, <i>hash(key0) % 4 = 1 </i>means a client must contact server 1 to fetch the cached data. Figure 5-1 shows the distribution of keys based on Table 5-1.</p><p style="padding-left: 34pt;text-indent: 0pt;text-align: left;"><span><img width="532" height="320" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_002.jpg"/></span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">This approach works well when the size of the server pool is fixed, and the data distribution is even. However, problems arise when new servers are added, or existing servers are removed. For example, if server 1 goes offline, the size of the server pool becomes 3. Using the same hash function, we get the same hash value for a key. But applying modular operation gives us different server indexes because the number of servers is reduced by 1. We get the results as shown in Table 5-2 by applying <i>hash % 3</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="569" height="415" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_003.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Figure 5-2 shows the new distribution of keys based on Table 5-2.</p><p style="padding-left: 40pt;text-indent: 0pt;text-align: left;"><span><img width="509" height="360" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_004.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">As shown in Figure 5-2, most keys are redistributed, not just the ones originally stored in the offline server (server 1). This means that when server 1 goes offline, most cache clients will connect to the wrong servers to fetch data. This causes a storm of cache misses. Consistent hashing is an effective technique to mitigate this problem.</p><h2 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Consistent hashing</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Quoted from Wikipedia: &quot;Consistent hashing is a special kind of hashing such that when a hash table is re-sized and consistent hashing is used, only k/n keys need to be remapped on average, where k is the number of keys, and n is the number of slots. In contrast, in most traditional hash tables, a change in the number of array slots causes nearly all keys to be remapped [1]”.</p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Hash space and hash ring</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Now we understand the definition of consistent hashing, let us find out how it works. Assume SHA-1 is used as the hash function f, and the output range of the hash function is: <i>x0, x1, x2, x3, …, xn</i>. In cryptography, SHA-1’s hash space goes from 0 to 2^160 - 1. That means <i>x0 </i>corresponds to 0, <i>xn </i>corresponds to 2^160 – 1, and all the other hash values in the middle fall between 0 and 2^160 - 1. Figure 5-3 shows the hash space.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="104" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_005.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">By collecting both ends, we get a hash ring as shown in Figure 5-4:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 152pt;text-indent: 0pt;text-align: left;"><span><img width="210" height="293" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_006.jpg"/></span></p><h2 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Hash servers</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Using the same hash function f, we map servers based on server IP or name onto the ring. Figure 5-5 shows that 4 servers are mapped on the hash ring.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="410" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_007.jpg"/></span></p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Hash keys</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">One thing worth mentioning is that hash function used here is different from the one in “the rehashing problem,” and there is no modular operation. As shown in Figure 5-6, 4 cache keys (key0, key1, key2, and key3) are hashed onto the hash ring</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="376" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_008.jpg"/></span></p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Server lookup</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">To determine which server a key is stored on, we go clockwise from the key position on the ring until a server is found. Figure 5-7 explains this process. Going clockwise, <i>key0 </i>is stored on <i>server 0</i>; <i>key1 </i>is stored on <i>server 1</i>; <i>key2 </i>is stored on <i>server 2 </i>and <i>key3 </i>is stored on <i>server 3</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span><img width="477" height="317" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_009.jpg"/></span></p><h2 style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Add a server</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Using the logic described above, adding a new server will only require redistribution of a fraction of keys.</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">In Figure 5-8, after a new <i>server 4 </i>is added, only <i>key0 </i>needs to be redistributed. <i>k1, k2, </i>and <i>k3 </i>remain on the same servers. Let us take a close look at the logic. Before <i>server 4 </i>is added, <i>key0 </i>is stored on <i>server 0</i>. Now, <i>key0 </i>will be stored on <i>server 4 </i>because <i>server 4 </i>is the first server it encounters by going clockwise from <i>key0</i>’s position on the ring. The other keys are not redistributed based on consistent hashing algorithm.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="443" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_010.jpg"/></span></p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Remove a server</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">When a server is removed, only a small fraction of keys require redistribution with consistent hashing. In Figure 5-9, when <i>server 1 </i>is removed, only <i>key1 </i>must be remapped to <i>server 2</i>.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">The rest of the keys are unaffected.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="431" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_011.jpg"/></span></p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Two issues in the basic approach</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The consistent hashing algorithm was introduced by Karger et al. at MIT [1]. The basic steps are:</p><ul id="l1"><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Map servers and keys on to the ring using a uniformly distributed hash function.</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">To find out which server a key is mapped to, go clockwise from the key position until the first server on the ring is found.</p></li></ul><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Two problems are identified with this approach. First, it is impossible to keep the same size of partitions on the ring for all servers considering a server can be added or removed. A partition is the hash space between adjacent servers. It is possible that the size of the partitions on the ring assigned to each server is very small or fairly large. In Figure 5-10, if <i>s1 </i>is removed, <i>s2’s </i>partition (highlighted with the bidirectional arrows) is twice as large as <i>s0 </i>and <i>s3’s </i>partition.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="351" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_012.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Second, it is possible to have a non-uniform key distribution on the ring. For instance, if servers are mapped to positions listed in Figure 5-11, most of the keys are stored on <i>server 2</i>. However, <i>server 1 </i>and <i>server 3 </i>have no data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="600" height="389" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_013.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">A technique called virtual nodes or replicas is used to solve these problems.</p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Virtual nodes</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">A virtual node refers to the real node, and each server is represented by multiple virtual nodes on the ring. In Figure 5-12, both <i>server 0 </i>and <i>server 1 </i>have 3 virtual nodes. The 3 is</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">arbitrarily chosen; and in real-world systems, the number of virtual nodes is much larger. Instead of using <i>s0</i>, we have <i>s0_0, s0_1</i>, and s0_2 to represent <i>server 0 </i>on the ring. Similarly, <i>s1_0, s1_1</i>, and <i>s1_2 </i>represent server 1 on the ring. With virtual nodes, each server is responsible for multiple partitions. Partitions (edges) with label <i>s0 </i>are managed by server 0. On the other hand, partitions with label <i>s1 </i>are managed by <i>server 1</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="450" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_014.jpg"/></span></p><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">To find which server a key is stored on, we go clockwise from the key’s location and find the first virtual node encountered on the ring. In Figure 5-13, to find out which server <i>k0 </i>is stored on, we go clockwise from <i>k0</i>’s location and find virtual node <i>s1_1</i>, which refers to <i>server 1</i>.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="390" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_015.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">As the number of virtual nodes increases, the distribution of keys becomes more balanced. This is because the standard deviation gets smaller with more virtual nodes, leading to balanced data distribution. Standard deviation measures how data are spread out. The outcome of an experiment carried out by online research [2] shows that with one or two hundred virtual nodes, the standard deviation is between 5% (200 virtual nodes) and 10% (100 virtual nodes) of the mean. The standard deviation will be smaller when we increase the number of virtual nodes. However, more spaces are needed to store data about virtual nodes. This is a tradeoff, and we can tune the number of virtual nodes to fit our system requirements.</p><h2 style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Find affected keys</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">When a server is added or removed, a fraction of data needs to be redistributed. How can we find the affected range to redistribute the keys?</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">In Figure 5-14, <i>server 4 </i>is added onto the ring. The affected range starts from <i>s4 </i>(newly added node) and moves anticlockwise around the ring until a server is found (<i>s3</i>). Thus, keys located between <i>s3 </i>and <i>s4 </i>need to be redistributed to <i>s4</i>.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="602" height="443" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_016.jpg"/></span></p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">When a server (<i>s1</i>) is removed as shown in Figure 5-15, the affected range starts from <i>s1 </i>(removed node) and moves anticlockwise around the ring until a server is found (<i>s0</i>). Thus, keys located between <i>s0 </i>and <i>s1 </i>must be redistributed to <i>s2</i>.</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><span><img width="596" height="429" alt="image" src="Alex5%20Consistent%20Hashing_files/Image_017.jpg"/></span></p><h2 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Wrap up</h2><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">In this chapter, we had an in-depth discussion about consistent hashing, including why it is needed and how it works. The benefits of consistent hashing include:</p><ul id="l2"><li data-list-text="•"><p style="padding-top: 2pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Minimized keys are redistributed when servers are added or removed.</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">It is easy to scale horizontally because data are more evenly distributed.</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 18pt;text-indent: 0pt;text-align: left;">Mitigate hotspot key problem. Excessive access to a specific shard could cause server overload. Imagine data for Katy Perry, Justin Bieber, and Lady Gaga all end up on the same shard. Consistent hashing helps to mitigate the problem by distributing the data more evenly.</p><p style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Consistent hashing is widely used in real-world systems, including some notable ones:</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Partitioning component of Amazon’s Dynamo database [3]</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Data partitioning across the cluster in Apache Cassandra [4]</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Discord chat application [5]</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Akamai content delivery network [6]</p></li><li data-list-text="•"><p style="padding-top: 3pt;padding-left: 26pt;text-indent: -7pt;text-align: left;">Maglev network load balancer [7]</p></li></ul><p style="padding-top: 6pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Congratulations on getting this far! Now give yourself a pat on the back. Good job!</p><h1 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Reference materials</h1><ol id="l3"><li data-list-text="[1]"><p style="padding-top: 5pt;padding-left: 21pt;text-indent: -17pt;text-align: left;"><a href="https://en.wikipedia.org/wiki/Consistent_hashing" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">Consistent hashing: </a><a href="https://en.wikipedia.org/wiki/Consistent_hashing" target="_blank">https://en.wikipedia.org/wiki/Consistent_hashing</a></p></li><li data-list-text="[2]"><p style="padding-top: 3pt;padding-left: 21pt;text-indent: -17pt;text-align: left;">Consistent Hashing:</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="https://tom-e-white.com/2007/11/consistent-hashing.html">https://tom-e-white.com/2007/11/consistent-hashing.html</a></p></li><li data-list-text="[3]"><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">Dynamo: Amazon’s Highly Available Key-value Store: </a><a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank">https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf</a></p></li><li data-list-text="[4]"><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 126%;text-align: left;"><a href="http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">Cassandra - A Decentralized Structured Storage System: </a><a href="http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF" target="_blank">http://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF</a></p></li><li data-list-text="[5]"><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="https://blog.discord.com/scaling-elixir-f9b8e1e7c29b" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">How Discord Scaled Elixir to 5,000,000 Concurrent Users: </a><a href="https://blog.discord.com/scaling-elixir-f9b8e1e7c29b" target="_blank">https://blog.discord.com/scaling-elixir-f9b8e1e7c29b</a></p></li><li data-list-text="[6]"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="http://theory.stanford.edu/%7Etim/s16/l/l1.pdf" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">CS168: The Modern Algorithmic Toolbox Lecture #1: Introduction and Consistent Hashing: </a><a href="http://theory.stanford.edu/%7Etim/s16/l/l1.pdf" target="_blank">http://theory.stanford.edu/~tim/s16/l/l1.pdf</a></p></li><li data-list-text="[7]"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf" style=" color: black; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank">Maglev: A Fast and Reliable Software Network Load Balancer: </a><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf" target="_blank">https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf</a></p></li></ol></body></html>
